Network Layer:

The initial architecture of the network layer utilizes two buffers.
Sending threads push to a buffer that receiving threads pull from,
and receiving threads push to a buffer that sending threads pull from.

This version has been synchronized and works well as long as the
buffer does not become entirely full, in which case packets are dropped
when attempting to send.

The problem with this approach is that it will not scale. All threads
need to serialize behind the buffer locks, which means the vast majority
of the time on the trace is spent in enter and leave critical section.
(e.g. send_packet has a count of 14 in my trace, 8 of which is
on EnterCriticalSection, 2 on CritSec contended and leave critsec.
receive_packet has a count of 36, of which 33 is enter/leave critsec.

All in all, 41/50 = 82% of computation is spent on this single wire
lock contention. This motivates the need for a more elegant and scalable
approach so the only limitations will be the bandwidth and latency of
the system, NOT the architecture of the code powering the simulation.

~~~~~~~~~~~~~~~~~~~~~~~~~~

After implementing a new design, I see huge improvement in my performance
trace! There is no evidence of contention now. The packets are moving
very fast. The primary limitation at this point in time is the capacity
of the NIC on the receiving end -- it seems that the NIC always is filled
too fast for the receiving thread to pull out all the packets. But that's okay --
I am sure we can expand the buffer size or improve how the students call receive_pkt.

One thing of note: the trace does show that the wire to NIC thread can
improve. It seems to wait too long in between runs. I wonder if it is scanning
through network space inefficiently? Perhaps we can skip 64 slots at a time!
